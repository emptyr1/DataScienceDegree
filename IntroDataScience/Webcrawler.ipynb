{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Define a procedure 'crawl_web' that takes as input a seed page url, and outputs a list of all\n",
      "#the urls that can be reached by following links starting from the seed page\n",
      "#seed will be list\n",
      "\n",
      "import urllib2\n",
      "def get_page(url):\n",
      "    try:\n",
      "        if url == \"http://xkcd.com/353\":\n",
      "            return urllib2.urlopen(url).read()\n",
      "        elif url == \"http://xkcd.com/554\":\n",
      "            return urllib2.urlopen(url).read()\n",
      "    except:\n",
      "        return \"\"\n",
      "    return \"\"\n",
      "\n",
      "\n",
      "def crawl_web(seed, max_depth):\n",
      "    tocrawl = [seed]  #seed here is just the url, hence only 1 item in the  tocrawl list initially\n",
      "    crawled = []\n",
      "    #max_traverse = 0\n",
      "    next_depth = []\n",
      "    depth = 0\n",
      "    while tocrawl and depth <= max_depth:\n",
      "        page = tocrawl.pop()\n",
      "        if page not in crawled:\n",
      "            union(next_depth, get_all_links(get_page(page)))\n",
      "            crawled.append(page)\n",
      "        if not tocrawl:\n",
      "            tocrawl, next_depth = next_depth, []\n",
      "            depth += 1\n",
      "    return crawled \n",
      "\n",
      "def union(p, q):\n",
      "    for e in q:\n",
      "        if e not in p:\n",
      "            p.append(e)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "def get_all_links(page):     #the goal of this function is to return a list containing all links\n",
      "    links = [] \n",
      "    while True:\n",
      "        url, endPos = get_next_target(page)\n",
      "        if url:\n",
      "            links.append()\n",
      "            page = page[endPos: ]\n",
      "        else:\n",
      "            break\n",
      "    return links\n",
      "\n",
      "            \n",
      "def get_next_target(page):  #page here is the actual html content\n",
      "    \n",
      "    start_link = page.find(\"<a href=\")\n",
      "    if start_link == -1:\n",
      "        return None, 0\n",
      "    start_quote = page.find('\"', start_link)\n",
      "    end_quote = page.find('\"', start_link+1)\n",
      "    url = page[start_quote+1:end_quote]\n",
      "    return url, end_quote\n",
      "    \n",
      "    \n",
      "print crawl_web(\"http://www.udacity.com/cs101x/index.html\",3)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['http://www.udacity.com/cs101x/index.html']\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}