{
 "metadata": {
  "name": "",
  "signature": "sha256:0e8611f310c06b71639258c39252699dccd5410876461bc5bd3650a3fe918a16"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "CSV\n",
      "----\n",
      "task is to process the supplied file and use the csv module to extract data from it.\n",
      "The data comes from NREL (National Renewable Energy Laboratory) website. Each file\n",
      "contains information from one meteorological station, in particular - about amount of\n",
      "solar and wind energy for each hour of day.\n",
      "\n",
      "#DATASOURCE: http://rredc.nrel.gov/solar/old_data/nsrdb/1991-2005/tmy3/by_USAFN.html\n",
      "\n",
      "Note that the first line of the datafile is neither data entry, nor header. It is a line\n",
      "describing the data source. You should extract the name of the station from it.\n",
      "\n",
      "The data should be returned as a list of lists (not dictionaries).\n",
      "You can use the csv modules \"reader\" method to get data in such format.\n",
      "Another useful method is next() - to get the next line from the iterator.\n",
      "You should only change the parse_file function.\n",
      "\"\"\"\n",
      "import csv\n",
      "import os\n",
      "\n",
      "DATADIR = \"\"\n",
      "DATAFILE = \"data/745090.csv\"\n",
      "\n",
      "\n",
      "'''def parse_file(datafile):\n",
      "    name = \"\"\n",
      "    data = []\n",
      "    k = 0\n",
      "    \n",
      "    with open(datafile,'rb') as f:\n",
      "        rownum = 0\n",
      "        reader = csv.reader(f)\n",
      "        someList = []\n",
      "        for row in reader:\n",
      "            if rownum == 0:\n",
      "                title = row\n",
      "                name = title[1]\n",
      "            elif rownum == 1:\n",
      "                header = row\n",
      "                #print header\n",
      "            else:\n",
      "                colnum = 0      \n",
      "                data.append(row)\n",
      "                \n",
      "            rownum += 1\n",
      "        print data[2][5]\n",
      "            \n",
      "    return (name, data)\n",
      "\n",
      "'''\n",
      "def parse_file(datafile):\n",
      "    name = \"\"\n",
      "    data = []\n",
      "    with open(datafile,'rb') as f:\n",
      "        r = csv.reader(f)\n",
      "        name = r.next()[1]\n",
      "        header = r.next()\n",
      "        data = [row for row in r]\n",
      "        #print row\n",
      "    print name, header, row\n",
      "    return (name, data)\n",
      "\n",
      "\n",
      "def test():\n",
      "    datafile = os.path.join(DATADIR, DATAFILE)\n",
      "    name, data = parse_file(datafile)\n",
      "\n",
      "    #assert name == \"MOUNTAIN VIEW MOFFETT FLD NAS\"\n",
      "    #assert data[0][1] == \"01:00\"\n",
      "    #assert data[2][0] == \"01/01/2005\"\n",
      "    #assert data[2][5] == \"2\"\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    test()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MOUNTAIN VIEW MOFFETT FLD NAS ['Date (MM/DD/YYYY)', 'Time (HH:MM)', 'ETR (W/m^2)', 'ETRN (W/m^2)', 'GHI (W/m^2)', 'GHI source', 'GHI uncert (%)', 'DNI (W/m^2)', 'DNI source', 'DNI uncert (%)', 'DHI (W/m^2)', 'DHI source', 'DHI uncert (%)', 'GH illum (lx)', 'GH illum source', 'Global illum uncert (%)', 'DN illum (lx)', 'DN illum source', 'DN illum uncert (%)', 'DH illum (lx)', 'DH illum source', 'DH illum uncert (%)', 'Zenith lum (cd/m^2)', 'Zenith lum source', 'Zenith lum uncert (%)', 'TotCld (tenths)', 'TotCld source', 'TotCld uncert (code)', 'OpqCld (tenths)', 'OpqCld source', 'OpqCld uncert (code)', 'Dry-bulb (C)', 'Dry-bulb source', 'Dry-bulb uncert (code)', 'Dew-point (C)', 'Dew-point source', 'Dew-point uncert (code)', 'RHum (%)', 'RHum source', 'RHum uncert (code)', 'Pressure (mbar)', 'Pressure source', 'Pressure uncert (code)', 'Wdir (degrees)', 'Wdir source', 'Wdir uncert (code)', 'Wspd (m/s)', 'Wspd source', 'Wspd uncert (code)', 'Hvis (m)', 'Hvis source', 'Hvis uncert (code)', 'CeilHgt (m)', 'CeilHgt source', 'CeilHgt uncert (code)', 'Pwat (cm)', 'Pwat source', 'Pwat uncert (code)', 'AOD (unitless)', 'AOD source', 'AOD uncert (code)', 'Alb (unitless)', 'Alb source', 'Alb uncert (code)', 'Lprecip depth (mm)', 'Lprecip quantity (hr)', 'Lprecip source', 'Lprecip uncert (code)'] ['01/05/2005', '02:00', '0', '0', '0', '2', '0', '0', '2', '0', '0', '2', '0', '0', '2', '0', '0', '2', '0', '0', '2', '0', '0', '2', '0', '10', 'E', '9', '10', 'E', '9', '8.0', 'A', '7', '6.0', 'A', '7', '87', 'A', '7', '1017', 'A', '7', '140', 'A', '7', '3.6', 'A', '7', '16100', 'A', '7', '2100', 'A', '7', '1.1', 'E', '8', '0.099', 'F', '8', '0.160', 'F', '8', '-9900', '-9900', '?', '0']\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# EXCEL FILE INPUT\n",
      "\n",
      "# -*- coding: utf-8 -*-\n",
      "# Finding the time and value of max load for each of the regions\n",
      "# COAST, EAST, FAR_WEST, NORTH, NORTH_C, SOUTHERN, SOUTH_C, WEST\n",
      "# and writing the result out in a csv file, using pipe character | as the delimiter.\n",
      "# \n",
      "\n",
      "import xlrd\n",
      "import os\n",
      "import csv\n",
      "from zipfile import ZipFile\n",
      "\n",
      "datafile = \"2013_ERCOT_Hourly_Load_Data.xls\"\n",
      "outfile = \"2013_Max_Loads.csv\"\n",
      "\n",
      "\n",
      "def open_zip(datafile):\n",
      "    with ZipFile('{0}.zip'.format(datafile), 'r') as myzip:\n",
      "        myzip.extractall()\n",
      "\n",
      "\n",
      "def parse_file(datafile):\n",
      "    workbook = xlrd.open_workbook(datafile)\n",
      "    sheet = workbook.sheet_by_index(0)\n",
      "    data = {} #empty dictionary\n",
      "    \n",
      "    for n in range(1, 9):\n",
      "        station = sheet.cell_value(0, n) #print the row right?\n",
      "        cv = sheet.col_values(n, start_rowx = 1, end_rowx = None) #all c\n",
      "        maxval = max(cv)\n",
      "        maxpos = maxval + 1\n",
      "        maxtime = \n",
      "        \n",
      "        \n",
      "    for row in range(sheet.nrows):\n",
      "        for col in range(sheet.ncols):\n",
      "            region = sheet.cell_value(row, 0)\n",
      "    \n",
      "    #region = sheet.cell_value\n",
      "    time = xlrd.xldate_as_tuple()\n",
      "    data = { region }\n",
      "    # Remember that you can use xlrd.xldate_as_tuple(sometime, 0) to convert\n",
      "    # Excel date to Python tuple of (year, month, day, hour, minute, second)\n",
      "    return data\n",
      "\n",
      "def save_file(data, filename):\n",
      "    # YOUR CODE HERE\n",
      "\n",
      "    \n",
      "def test():\n",
      "    open_zip(datafile)\n",
      "    data = parse_file(datafile)\n",
      "    save_file(data, outfile)\n",
      "\n",
      "    number_of_rows = 0\n",
      "    stations = []\n",
      "\n",
      "    ans = {'FAR_WEST': {'Max Load': '2281.2722140000024',\n",
      "                        'Year': '2013',\n",
      "                        'Month': '6',\n",
      "                        'Day': '26',\n",
      "                        'Hour': '17'}}\n",
      "    correct_stations = ['COAST', 'EAST', 'FAR_WEST', 'NORTH',\n",
      "                        'NORTH_C', 'SOUTHERN', 'SOUTH_C', 'WEST']\n",
      "    fields = ['Year', 'Month', 'Day', 'Hour', 'Max Load']\n",
      "\n",
      "    with open(outfile) as of:\n",
      "        csvfile = csv.DictReader(of, delimiter=\"|\")\n",
      "        for line in csvfile:\n",
      "            station = line['Station']\n",
      "            if station == 'FAR_WEST':\n",
      "                for field in fields:\n",
      "                    # Check if 'Max Load' is within .1 of answer\n",
      "                    if field == 'Max Load':\n",
      "                        max_answer = round(float(ans[station][field]), 1)\n",
      "                        max_line = round(float(line[field]), 1)\n",
      "                        assert max_answer == max_line\n",
      "\n",
      "                    # Otherwise check for equality\n",
      "                    else:\n",
      "                        assert ans[station][field] == line[field]\n",
      "\n",
      "            number_of_rows += 1\n",
      "            stations.append(station)\n",
      "\n",
      "        # Output should be 8 lines not including header\n",
      "        assert number_of_rows == 8\n",
      "\n",
      "        # Check Station Names\n",
      "        assert set(stations) == set(correct_stations)\n",
      "\n",
      "        \n",
      "if __name__ == \"__main__\":\n",
      "    test()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#JSON\n",
      "#======\n",
      "\n",
      "#!/usr/bin/env python\n",
      "# -*- coding: utf-8 -*-\n",
      "\"\"\"\n",
      "This exercise shows some important concepts:\n",
      "- using codecs module to write unicode files\n",
      "- using authentication with web APIs\n",
      "- using offset when accessing web APIs\n",
      "\n",
      "To run this code locally you have to register at the NYTimes developer site \n",
      "and get your own API key. You will be able to complete this exercise in our UI without doing so,\n",
      "as we have provided a sample result.\n",
      "\n",
      "Your task is to process the saved file that represents the most popular (by view count)\n",
      "articles in the last day, and return the following data:\n",
      "- list of dictionaries, where the dictionary key is \"section\" and value is \"title\"\n",
      "- list of URLs for all media entries with \"format\": \"Standard Thumbnail\"\n",
      "\n",
      "All your changes should be in the article_overview function.\n",
      "The rest of functions are provided for your convenience, if you want to access the API by yourself.\n",
      "\"\"\"\n",
      "import json\n",
      "import codecs\n",
      "import requests\n",
      "\n",
      "URL_MAIN = \"http://api.nytimes.com/svc/\"\n",
      "URL_POPULAR = URL_MAIN + \"mostpopular/v2/\"\n",
      "API_KEY = { \"popular\": \"0186e8182b85c3c0312bc1020a1ef43c:7:66894836\",\n",
      "            \"article\": \"f5e77500864c47668a2a85424927cf12:17:66894836\"}\n",
      "\n",
      "\n",
      "def get_from_file(kind, period):\n",
      "    filename = \"popular-{0}-{1}.json\".format(kind, period)\n",
      "    with open(filename, \"r\") as f:\n",
      "        return json.loads(f.read())\n",
      "\n",
      "\n",
      "def article_overview(kind, period):\n",
      "    data = get_from_file(kind, period)\n",
      "    titles = []\n",
      "    urls =[]\n",
      "    for article in data:\n",
      "        section = article[\"section\"]\n",
      "        title = article[\"title\"]\n",
      "        titles.append({section : title})\n",
      "        if \"media\" in article:\n",
      "            for m in article[\"media\"]:\n",
      "                print m\n",
      "\n",
      "    return (titles, urls)\n",
      "\n",
      "\n",
      "def query_site(url, target, offset):\n",
      "    # This will set up the query with the API key and offset\n",
      "    # Web services often use offset paramter to return data in small chunks\n",
      "    # NYTimes returns 20 articles per request, if you want the next 20\n",
      "    # You have to provide the offset parameter\n",
      "    if API_KEY[\"popular\"] == \"0186e8182b85c3c0312bc1020a1ef43c:7:66894836\" or API_KEY[\"article\"] == \"f5e77500864c47668a2a85424927cf12:17:66894836\":\n",
      "        print \"You need to register for NYTimes Developer account to run this program.\"\n",
      "        print \"See Intructor notes for information\"\n",
      "        return False\n",
      "    params = {\"api-key\": API_KEY[target], \"offset\": offset}\n",
      "    r = requests.get(url, params = params)\n",
      "\n",
      "    if r.status_code == requests.codes.ok:\n",
      "        return r.json()\n",
      "    else:\n",
      "        r.raise_for_status()\n",
      "\n",
      "\n",
      "def get_popular(url, kind, days, section=\"all-sections\", offset=0):\n",
      "    # This function will construct the query according to the requirements of the site\n",
      "    # and return the data, or print an error message if called incorrectly\n",
      "    if days not in [1,7,30]:\n",
      "        print \"Time period can be 1,7, 30 days only\"\n",
      "        return False\n",
      "    if kind not in [\"viewed\", \"shared\", \"emailed\"]:\n",
      "        print \"kind can be only one of viewed/shared/emailed\"\n",
      "        return False\n",
      "\n",
      "    url = URL_POPULAR + \"most{0}/{1}/{2}.json\".format(kind, section, days)\n",
      "    data = query_site(url, \"popular\", offset)\n",
      "\n",
      "    return data\n",
      "\n",
      "\n",
      "def save_file(kind, period):\n",
      "    # This will process all results, by calling the API repeatedly with supplied offset value,\n",
      "    # combine the data and then write all results in a file.\n",
      "    data = get_popular(URL_POPULAR, \"viewed\", 1)\n",
      "    num_results = data[\"num_results\"]\n",
      "    full_data = []\n",
      "    with codecs.open(\"popular-{0}-{1}-full.json\".format(kind, period), encoding='utf-8', mode='w') as v:\n",
      "        for offset in range(0, num_results, 20):        \n",
      "            data = get_popular(URL_POPULAR, kind, period, offset=offset)\n",
      "            full_data += data[\"results\"]\n",
      "        \n",
      "        v.write(json.dumps(full_data, indent=2))\n",
      "\n",
      "\n",
      "def test():\n",
      "    titles, urls = article_overview(\"viewed\", 1)\n",
      "    assert len(titles) == 20\n",
      "    assert len(urls) == 30\n",
      "    assert titles[2] == {'Opinion': 'Professors, We Need You!'}\n",
      "    assert urls[20] == 'http://graphics8.nytimes.com/images/2014/02/17/sports/ICEDANCE/ICEDANCE-thumbStandard.jpg'\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    test()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IOError",
       "evalue": "[Errno 2] No such file or directory: 'popular-viewed-1.json'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-33-845d5fb68fb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-33-845d5fb68fb9>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mtitles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marticle_overview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"viewed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitles\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-33-845d5fb68fb9>\u001b[0m in \u001b[0;36marticle_overview\u001b[0;34m(kind, period)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0marticle_overview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mtitles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0murls\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-33-845d5fb68fb9>\u001b[0m in \u001b[0;36mget_from_file\u001b[0;34m(kind, period)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"popular-{0}-{1}.json\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'popular-viewed-1.json'"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "LESSON 2: DATA IN MORE COMPLEX FORMATS: \n",
      "=======\n",
      "XML\n",
      "=======\n",
      "'''\n",
      "\n",
      "#!/usr/bin/env python\n",
      "# task here is to extract data from xml on authors of an article\n",
      "# and add it to a list, one item for an author.\n",
      "# See the provided data structure for the expected format.\n",
      "# The tags for first name, surname and email should map directly\n",
      "# to the dictionary keys\n",
      "import xml.etree.ElementTree as ET\n",
      "\n",
      "article_file = \"data/exampleResearchArticle.xml\"\n",
      "\n",
      "\n",
      "def get_root(fname):\n",
      "    tree = ET.parse(fname)\n",
      "    return tree.getroot()\n",
      "\n",
      "\n",
      "def get_authors(root):\n",
      "    authors = []\n",
      "    for author in root.findall('./fm/bibl/aug/au'):\n",
      "        data = {\n",
      "                \"fnm\": None,\n",
      "                \"snm\": None,\n",
      "                \"email\": None,\n",
      "                 \"insr\":[]\n",
      "        }\n",
      "        print 'asd'\n",
      "\n",
      "        data[\"email\"] = author.find('./email').text\n",
      "        data[\"fnm\"] = author.find('./fnm').text\n",
      "        data[\"snm\"] = author.find('./snm').text\n",
      "        insr = author.findall('./insr')\n",
      "        for i in insr:\n",
      "            data[\"insr\"].append(i.attrib[\"iid\"])\n",
      "                \n",
      "        authors.append(data)\n",
      "    return authors\n",
      "    print authors\n",
      "\n",
      "\n",
      "def test():\n",
      "    solution = [{'fnm': 'Omer', 'snm': 'Mei-Dan', 'email': 'omer@extremegate.com'}, {'fnm': 'Mike', 'snm': 'Carmont', 'email': 'mcarmont@hotmail.com'}, {'fnm': 'Lior', 'snm': 'Laver', 'email': 'laver17@gmail.com'}, {'fnm': 'Meir', 'snm': 'Nyska', 'email': 'nyska@internet-zahav.net'}, {'fnm': 'Hagay', 'snm': 'Kammar', 'email': 'kammarh@gmail.com'}, {'fnm': 'Gideon', 'snm': 'Mann', 'email': 'gideon.mann.md@gmail.com'}, {'fnm': 'Barnaby', 'snm': 'Clarck', 'email': 'barns.nz@gmail.com'}, {'fnm': 'Eugene', 'snm': 'Kots', 'email': 'eukots@gmail.com'}]\n",
      "    \n",
      "    root = get_root(article_file)\n",
      "    data = get_authors(root)\n",
      "\n",
      "    print data\n",
      "\n",
      "    #assert data[0] == solution[0]\n",
      "    #assert data[1][\"fnm\"] == solution[1][\"fnm\"]\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    test()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[]\n"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/env python\n",
      "# -*- coding: utf-8 -*-\n",
      "'''\n",
      "HTML WITH GET/POST REQUEST TO: http://www.transtats.bts.gov/Data_Elements.aspx?Data=2\n",
      "===================\n",
      "\n",
      "'''\n",
      "# Task is to process the HTML using BeautifulSoup, extract the hidden\n",
      "# form field values for \"__EVENTVALIDATION\" and \"__VIEWSTATE\" and set the approprate\n",
      "# values in the data dictionary.\n",
      "\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import json\n",
      "\n",
      "html_page = \"page_source.html\"\n",
      "\n",
      "\n",
      "def extract_data(page):\n",
      "    data = {\"eventvalidation\": \"\",\n",
      "            \"viewstate\": \"\"}\n",
      "    with open(page, \"r\") as html:\n",
      "        soup = BeautifulSoup(html)\n",
      "        # do something here to find the necessary values\n",
      "        ev = soup.find(id = '__EVENTVALIDATION')\n",
      "        vs = soup.find(id = '__VIEWSTATE')\n",
      "        data[\"eventvalidation\"] = ev[\"value\"]\n",
      "        data[\"viewstate\"] = vs[\"value\"]\n",
      "        pass\n",
      "\n",
      "    return data\n",
      "\n",
      "\n",
      "def make_request(data):\n",
      "    eventvalidation = data[\"eventvalidation\"]\n",
      "    viewstate = data[\"viewstate\"]\n",
      "\n",
      "    r = requests.post(\"http://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
      "                    data={'AirportList': \"BOS\",\n",
      "                          'CarrierList': \"VX\",\n",
      "                          'Submit': 'Submit',\n",
      "                          \"__EVENTTARGET\": \"\",\n",
      "                          \"__EVENTARGUMENT\": \"\",\n",
      "                          \"__EVENTVALIDATION\": eventvalidation,\n",
      "                          \"__VIEWSTATE\": viewstate\n",
      "                    })\n",
      "\n",
      "    return r.text\n",
      "\n",
      "\n",
      "def test():\n",
      "    data = extract_data(html_page)\n",
      "    assert data[\"eventvalidation\"] != \"\"\n",
      "    assert data[\"eventvalidation\"].startswith(\"/wEWjAkCoIj1ng0\")\n",
      "    assert data[\"viewstate\"].startswith(\"/wEPDwUKLTI\")\n",
      "\n",
      "    \n",
      "test()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "DataExtraction_1.ipynb  \u001b[1m\u001b[34mdata\u001b[m\u001b[m/\r\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/env python\n",
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "'''\n",
      "HTML WITH GET/POST REQUEST TO: http://www.transtats.bts.gov/Data_Elements.aspx?Data=2\n",
      "===================\n",
      "\n",
      "'''\n",
      "# Let's assume that you combined the code from the previous 2 exercises\n",
      "# with code from the lesson on how to build requests, and downloaded all the data locally.\n",
      "# The files are in a directory \"data\", named after the carrier and airport:\n",
      "# \"{}-{}.html\".format(carrier, airport), for example \"FL-ATL.html\".\n",
      "# The table with flight info has a table class=\"dataTDRight\".\n",
      "# There are couple of helper functions to deal with the data files.\n",
      "# Please do not change them for grading purposes.\n",
      "# All your changes should be in the 'process_file' function\n",
      "# This is example of the datastructure you should return\n",
      "# Each item in the list should be a dictionary containing all the relevant data\n",
      "# Note - year, month, and the flight data should be integers\n",
      "# You should skip the rows that contain the TOTAL data for a year\n",
      "# data = [{\"courier\": \"FL\",\n",
      "#         \"airport\": \"ATL\",\n",
      "#         \"year\": 2012,\n",
      "#         \"month\": 12,\n",
      "#         \"flights\": {\"domestic\": 100,\n",
      "#                     \"international\": 100}\n",
      "#         },\n",
      "#         {\"courier\": \"...\"}\n",
      "# ]\n",
      "from bs4 import BeautifulSoup\n",
      "from zipfile import ZipFile\n",
      "import os\n",
      "\n",
      "datadir = \"data\"\n",
      "\n",
      "\n",
      "def open_zip(datadir):\n",
      "    with ZipFile('{0}.zip'.format(datadir), 'r') as myzip:\n",
      "        myzip.extractall()\n",
      "\n",
      "\n",
      "def process_all(datadir):\n",
      "    files = os.listdir(datadir)\n",
      "    return files\n",
      "\n",
      "\n",
      "def process_file(f):\n",
      "    # This is example of the datastructure you should return\n",
      "    # Each item in the list should be a dictionary containing all the relevant data\n",
      "    # Note - year, month, and the flight data should be integers\n",
      "    # You should skip the rows that contain the TOTAL data for a year\n",
      "    # data = [{\"courier\": \"FL\",\n",
      "    #         \"airport\": \"ATL\",\n",
      "    #         \"year\": 2012,\n",
      "    #         \"month\": 12,\n",
      "    #         \"flights\": {\"domestic\": 100,\n",
      "    #                     \"international\": 100}\n",
      "    #         },\n",
      "    #         {\"courier\": \"...\"}\n",
      "    # ]\n",
      "    data = []\n",
      "    info = {}\n",
      "    info[\"courier\"], info[\"airport\"] = f[:6].split(\"-\")\n",
      "    \n",
      "    with open(\"{}/{}\".format(datadir, f), \"r\") as html:\n",
      "\n",
      "        soup = BeautifulSoup(html)\n",
      "        \n",
      "\n",
      "    return data\n",
      "\n",
      "\n",
      "def test():\n",
      "    print \"Running a simple test...\"\n",
      "    open_zip(datadir)\n",
      "    files = process_all(datadir)\n",
      "    data = []\n",
      "    for f in files:\n",
      "        data += process_file(f)\n",
      "    assert len(data) == 399\n",
      "    for entry in data[:3]:\n",
      "        assert type(entry[\"year\"]) == int\n",
      "        assert type(entry[\"month\"]) == int\n",
      "        assert type(entry[\"flights\"][\"domestic\"]) == int\n",
      "        assert len(entry[\"airport\"]) == 3\n",
      "        assert len(entry[\"courier\"]) == 2\n",
      "    assert data[-1][\"airport\"] == \"ATL\"\n",
      "    assert data[-1][\"flights\"] == {'international': 108289, 'domestic': 701425}\n",
      "    \n",
      "    print \"... success!\"\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    test()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/env python\n",
      "# -*- coding: utf-8 -*-\n",
      "# Let's assume that you combined the code from the previous 2 exercises\n",
      "# with code from the lesson on how to build requests, and downloaded all the data locally.\n",
      "# The files are in a directory \"data\", named after the carrier and airport:\n",
      "# \"{}-{}.html\".format(carrier, airport), for example \"FL-ATL.html\".\n",
      "# The table with flight info has a table class=\"dataTDRight\".\n",
      "# There are couple of helper functions to deal with the data files.\n",
      "# Please do not change them for grading purposes.\n",
      "# All your changes should be in the 'process_file' function\n",
      "# This is example of the datastructure you should return\n",
      "# Each item in the list should be a dictionary containing all the relevant data\n",
      "# Note - year, month, and the flight data should be integers\n",
      "# You should skip the rows that contain the TOTAL data for a year\n",
      "# data = [{\"courier\": \"FL\",\n",
      "#         \"airport\": \"ATL\",\n",
      "#         \"year\": 2012,\n",
      "#         \"month\": 12,\n",
      "#         \"flights\": {\"domestic\": 100,\n",
      "#                     \"international\": 100}\n",
      "#         },\n",
      "#         {\"courier\": \"...\"}\n",
      "# ]\n",
      "from bs4 import BeautifulSoup\n",
      "from zipfile import ZipFile\n",
      "import os\n",
      "\n",
      "datadir = \"data\"\n",
      "\n",
      "\n",
      "def open_zip(datadir):\n",
      "    with ZipFile('{0}.zip'.format(datadir), 'r') as myzip:\n",
      "        myzip.extractall()\n",
      "\n",
      "\n",
      "def process_all(datadir):\n",
      "    files = os.listdir(datadir)\n",
      "    return files\n",
      "\n",
      "\n",
      "def process_file(f):\n",
      "    # Note - year, month, and the flight data should be integers\n",
      "    # You should skip the rows that contain the TOTAL data for a year\n",
      "    # data = [{\"courier\": \"FL\",\n",
      "    #         \"airport\": \"ATL\",\n",
      "    #         \"year\": 2012,\n",
      "    #         \"month\": 12,\n",
      "    #         \"flights\": {\"domestic\": 100,\n",
      "    #                     \"international\": 100}\n",
      "    #         },\n",
      "    #         {\"courier\": \"...\"}\n",
      "    # ]\n",
      "    data = []\n",
      "    info = {}\n",
      "    \n",
      "    info[\"courier\"], info[\"airport\"] = f[:6].split(\"-\")\n",
      "    \n",
      "    with open(\"{}/{}\".format(datadir, f), \"r\") as html:\n",
      "        \n",
      "        soup = BeautifulSoup(html)\n",
      "        table = soup.find_all('table', attrs = 'dataTDRight')\n",
      "        tr_tags = soup.find_all('tr', attrs = 'dataTDRight')\n",
      "        \n",
      "        for tr in tr_tags:\n",
      "            tds = tr.find_all('td')\n",
      "            if tds[1].get_text() != 'TOTAL':\n",
      "                info['year'] = int(tds[0].get_text())\n",
      "                info['month'] = int(tds[1].get_text())\n",
      "                \n",
      "                flights = {}\n",
      "                flights['domestic'] = int(tds[2].get_text().replace(',', ''))\n",
      "                flights['international'] = int(tds[3].get_text().replace(',', ''))\n",
      "                info['flights'] = flights\n",
      "                \n",
      "                data.append(info)\n",
      "          \n",
      "    return data\n",
      "\n",
      "\n",
      "def test():\n",
      "    print \"Running a simple test...\"\n",
      "    open_zip(datadir)\n",
      "    files = process_all(datadir)\n",
      "    data = []\n",
      "    for f in files:\n",
      "        data += process_file(f)\n",
      "    assert len(data) == 399\n",
      "    for entry in data[:3]:\n",
      "        assert type(entry[\"year\"]) == int\n",
      "        assert type(entry[\"month\"]) == int\n",
      "        assert type(entry[\"flights\"][\"domestic\"]) == int\n",
      "        assert len(entry[\"airport\"]) == 3\n",
      "        assert len(entry[\"courier\"]) == 2\n",
      "    assert data[-1][\"airport\"] == \"ATL\"\n",
      "    assert data[-1][\"flights\"] == {'international': 108289, 'domestic': 701425}\n",
      "    \n",
      "    print \"... success!\"\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    test()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}